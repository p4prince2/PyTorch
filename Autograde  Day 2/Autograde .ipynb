{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b6d4d8-e32a-4f93-bb44-dbb78b4d685f",
   "metadata": {},
   "source": [
    "***PyTorch, the term autograde typically refers to automatic differentiation (or autograd), which is a key feature that enables PyTorch to automatically compute gradients for backpropagation during training of machine learning models. The Autograd module in PyTorch is designed to automatically compute the gradients of tensors during the forward pass, and this is crucial for training neural networks using optimization techniques like gradient descent.***\n",
    "\n",
    "**Key Concepts of Autograd in PyTorch:**\n",
    "\n",
    "**Tensors and Gradients:**\n",
    "\n",
    "- PyTorch provides a special class of tensors called torch.Tensor which can track computation history.\n",
    "\n",
    "- Tensors with requires_grad=True will track all operations applied to them and store the gradient for those operations.\n",
    "\n",
    "**Backward Pass:**\n",
    "\n",
    "- The .backward() method computes all gradients for the tensors involved in the computation graph.\n",
    "\n",
    "- After calling .backward(), gradients are stored in the .grad attribute of each tensor that requires gradients.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "- PyTorch builds a dynamic computation graph as you define operations. Each operation adds a node to the graph.\n",
    "\n",
    "- When you call .backward(), the system traverses the graph and computes gradients for each node.\n",
    "\n",
    "**Gradient Descent:**\n",
    "\n",
    "- Once gradients are computed, they can be used to update model parameters during training, usually through optimizers like torch.optim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a114921-c457-4e2f-b274-783fef05a017",
   "metadata": {},
   "source": [
    "## Example Code for Autograd in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42571b8d-4c7a-49bd-bd03-5df40683e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to differente in normal using python\n",
    "def dy_dx(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826b62ad-aa5f-45e1-afcb-440c3cc9c977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078e6ae-dda1-4413-bf37-c82b7a738a22",
   "metadata": {},
   "source": [
    "**By using tensor-autograde**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b798b8db-4f68-45a8-afb7-d21c804ac3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471488e7-3b5f-4394-b6b5-f86f4f92ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(3.0,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4d55146-d656-45f9-ae17-5eff5f8dca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f6ba8ab-9563-45e1-8a06-e3b6072c5a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfefdeee-5e66-48a6-b289-5d376f3a11ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea699dd-d048-4087-a480-fc4410fbca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ef58aa3-843c-4233-977e-db70fe1b471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad  ## dy_dx(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e4f723e-195b-4c65-98a6-8cd373cf4ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In one line\n",
    "X=torch.tensor(3.0,requires_grad=True)\n",
    "y=X**2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afcf0fc-c9d8-44a0-983b-076cfcc48336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27f86da0-dc6f-4ee7-a232-0cc5cd47c3a4",
   "metadata": {},
   "source": [
    "**Another without using Autograde**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f077697-aae6-4613-9d9a-f3eca8a7f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# y=x^2\n",
    "# z=sin(y)\n",
    "\n",
    "\n",
    "import math\n",
    "def dz_dx(x):\n",
    "    return 2* x* math.cos(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aee38bce-08b4-48e9-b033-5d8b1a70ea80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca8157-9d5b-45d6-8d81-cd9bd3eee21b",
   "metadata": {},
   "source": [
    "**with Autograde**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2de93cf-163c-469b-a9bc-cfac07c5111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(4.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5690a809-708a-4aba-b595-2a913e8700d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba30b31d-3b96-40a4-8b2b-add65b17631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ea98822-aa6e-474f-a062-344b97fa6390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8431049-ecee-432b-9d64-e0449d5c7657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "529dc5f6-538e-43c0-bbb0-0ba27751630c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2879, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cecdd61f-124c-4eac-a2ae-01f53808e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71c30c9c-2494-4bac-a17f-16debe31ad3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.6613)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89fb6269-add5-406d-9f08-25b3da9707a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.6613)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## in one line \n",
    "x=torch.tensor(4.0, requires_grad=True)\n",
    "y=x**2\n",
    "z=torch.sin(y)\n",
    "z.backward()\n",
    "x.grad  ## samee with dz_dx  but simple to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb165e6d-ed1d-45b7-be3d-52a2a9495a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p4pri\\AppData\\Local\\Temp\\ipykernel_28312\\486760323.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  y.grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95944a-2a88-4b3a-b81f-0ecfe5e11636",
   "metadata": {},
   "source": [
    "**Single perceptron without using Autograde**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f9f0251-c033-4573-8add-271f61a36ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(6.7)  # input feature \n",
    "y=torch.tensor(0.0)  ## True label (binary)\n",
    "\n",
    "w=torch.tensor(1.0) # weight\n",
    "b=torch.tensor(0.0)  # bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3224f06-5bde-44d9-9999-7ad512ed1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross-Entropy loss for scaler\n",
    "\n",
    "def binary_cross_entropy_loss(prediction,target):\n",
    "    epsilon=1e-8   # to prevent log(0)\n",
    "    prediction=torch.clamp(prediction,epsilon,1-epsilon)#torch.clamp(input, min=0, max=10)\n",
    "    return -(target*torch.log(prediction)+ (1 -target)*torch.log(1-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d3a3d59-bb2c-4f3f-904f-785cb593f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forwar pass\n",
    "\n",
    "z=w*x+b  # Weighted sum (liner part)\n",
    "y_pred=torch.sigmoid(z)  # predtion probability\n",
    "\n",
    "# compute binary cross-entropy_loss\n",
    "loss=binary_cross_entropy_loss(y_pred,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f62548b-5be8-45b5-9a1b-69060af26d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6d740f8-36dd-4004-8209-cc5c898020c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Derivaties:\n",
    "\n",
    "#1. dl/d(y_pred): Loss with respect to prediction(y_pred)\n",
    "dloss_dy_pred=(y_pred  -y )/(y_pred*(1-y_pred))\n",
    "\n",
    "#2. dy_pred/dz: Prediction(y_pred) with repect toz (sigmoid  derivatie)\n",
    "dy_pred_dz=y_pred*(1-y_pred)\n",
    "\n",
    "#3. dz/dw and dz/db : z with respect to w and b \n",
    "dz_dw = x #dz/dw=x\n",
    "dz_db=1 # dz/db=1(bias contribution directly to z)\n",
    "\n",
    "\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e715a7e-ca1c-4ea2-98f7-76979a595cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e1cad-b17b-4b91-88a2-859b5862c6f1",
   "metadata": {},
   "source": [
    "**Single perceptron with using Autograde**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "acb3dc8e-f960-4e29-b274-beb555f23aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.tensor(6.7)\n",
    "y=torch.tensor(0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9cc71b09-f3dd-4865-9d77-29fd2fa4b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=torch.tensor(1.0, requires_grad=True)\n",
    "b=torch.tensor(0.0,requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e4730b9-d340-488a-9fe1-9d8ea252772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "476a3fc4-c88e-4a4b-b740-8e648ebb01d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., requires_grad=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "29b684a9-d048-4824-a07a-29f97643de29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=w*x+b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cebb854-8414-40aa-9c40-2aa7578873ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28aeda3c-c9e0-4316-81a5-f294fe92689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=binary_cross_entropy_loss(y_pred,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7ac56e4-2605-4c51-880d-361674377e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7cc10b90-9d7c-48bc-aab8-8770e1da6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)   ## same as above   here we dont have to write the code for Derivaties:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae59b29-0dae-4c71-afcd-04f944a9d14d",
   "metadata": {},
   "source": [
    "**for vector value** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c87ca68-e664-4c52-8da1-d9c0885d5a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1.0,2.0,3.0],requires_grad=True)\n",
    "y=(x**2).mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2eb28032-2637-4478-901c-8e6691dc31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02b6f629-fff5-427b-8b15-808959df21ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ddff14-12ca-4445-b204-c2b7285c579c",
   "metadata": {},
   "source": [
    "# clearing grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d273e-3808-475b-99d9-b94b069d2e3e",
   "metadata": {},
   "source": [
    "**Why clearing grad  important**\n",
    "\n",
    "- Clearing gradients is important to avoid the accumulation of gradients across different iterations, ensure fresh gradient computation, and prevent unnecessary memory usage.\n",
    "\n",
    "- It guarantees that each iteration correctly updates the model parameters based on the current batch's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4eec41d6-5291-4964-a3dd-4dcec2e778f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b5890e99-9fab-432a-8326-51f96cc79816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c4459c2c-c801-4eff-9137-87bf851e7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5e1eb64e-7dcb-46fa-83cf-ddf819003109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2c79b57f-6f04-4e8f-98fd-aea0601b067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff265ecb-3cbb-4991-9157-12cf1dfea380",
   "metadata": {},
   "source": [
    "# Disabling gradient tracking\n",
    "Disabling gradient tracking is useful when you donâ€™t need to compute gradients for certain parts of the model, or during inference (prediction) where you don't want the computational graph to be built, which can save memory and computational resources. This is done using the torch.no_grad() context manager or by setting the requires_grad attribute of tensors to False.\n",
    "\n",
    "**option 1 - requires_grad_(False)**\n",
    "\n",
    "**option 2 - detach()**\n",
    "\n",
    "**option 3 - torch.no_grad()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "06d5cdac-cfc9-4f7a-a4b8-9fc6da264048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "697d8743-3789-4ad0-9680-34b6813335a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ad06d452-57e6-4806-a0c5-fcd0c00fe275",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "94d281d5-2b03-4d88-88c0-7b632a84cc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5b90df08-50f5-46c4-a9c1-227e43acb731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 - requires_grad_(False)\n",
    "# option 2 - detach()\n",
    "# option 3 - torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "146b68da-8102-49a7-b18d-f3715e98477f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# option 1 - requires_grad_(False)\n",
    "x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "959faedc-3eb6-49c4-9150-40e5df260024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fa678636-76d1-4aa4-9ef2-62726659461c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 2 - detach()\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "765e7550-875e-415d-9ba3-0ba05308c192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.detach()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1f499259-5a8a-4ef3-a11b-c85ec3b7d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# option 3 - torch.no_grad()\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "# Disable gradient tracking\n",
    "with torch.no_grad():\n",
    "    y = x * 2  # Operations here won't track gradients\n",
    "\n",
    "print(y.requires_grad)  # Output will be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cf8ee-40b7-4606-8371-2cb8d3942c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
